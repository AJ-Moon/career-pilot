[
  {
    "question": "Suppose you had bank transaction data, and wanted to separate out likely fraudulent transactions. How would you approach it? Why might accuracy be a bad metric for evaluating success?",
    "answer": "In Machine Learning, problems like fraud detection are usually framed as classification problems. In order to solve this problem we may use different features like amount, merchant, location, time etc associated with each transaction.\nOne of the biggest challenge with fraud transaction detection is- majority of transactions are not fraud, so we have inbalance data!\nFirst step will be to do EDA and understand our data and intesity of class inbalance.\nIn order to handle inbalance data problem we can use one of the following method\nOversampling — SMOTE (Synthetic Minority Over-sampling Technique)\nUndersampling — One simple way of undersampling is randomly selecting a handful of samples from the class that is overrepresented.\nCombined Class Methods — Use SMOTE together with edited nearest-neighbours (ENN). Here, ENN is used as the cleaning method after SMOTE over-sampling to obtain a cleaner space.\nDeveloped by Wilson (1972), the ENN method works by finding the K-nearest neighbor of each observation first, then check whether the majority class from the observation’s k-nearest neighbor is the same as the observation’s class or not. \nIf the majority class of the observation’s K-nearest neighbor and the observation’s class is different, then the observation and its K-nearest neighbor are deleted from the dataset. In default, the number of nearest-neighbor used in ENN is K=3.\nAs ENN removes the observation and its K-nearest neighbor instead of just removing observation and its 1-nearest neighbor that are having different classes. Thus, ENN can be expected to give more in-depth data cleaning.\nTest model performane for each of above technique and choose best performing model.\nWhy might accuracy be a bad metric for evaluating success? \nIn case of inbalance data accuracy metric is not usefull. Accuracy tells us how close a measured value is to the actual (true) value. But here we are more interested in fraud transactions.\nWe dont mind declaring few good transactions as fruad but failing to identify fraud transaction is not acceptable. In such cases classification of true positives is a priority, hence precision metric make more sense."
  },
  {
    "question": "What are the assumptions for linear regression",
    "answer": "Linear regression assumptions are as below\nData should have linear relationship between X and Y (actually mean of Y)\nData should be normally distributed\nNo or little multicollinearity (observations should be independent of each other)\nAssumption of additivity: This means that each feature (X) should affect the target (Y) independently. In other words, the influence of one feature on the target does not change because of another feature.\nExample: Let's say you're predicting house prices based on square footage and number of bedrooms. Additivity assumes that adding an extra bedroom increases the price by a certain amount, regardless of whether the house is large or small.If the impact of the bedroom varied based on house size (e.g., extra bedroom matters more in small houses), then the additivity assumption would be violated.\nHomoscedasticity: This means that the spread of the errors (the differences between your model's predictions and the actual values) should be roughly the same for all the predicted values of Y.\nExample: If you're predicting a person's weight based on their height, the spread of actual weights around the predicted weight should be similar whether the person is tall or short. If the spread varies widely (e.g., much larger for taller people), this assumption is violated."
  },
  {
    "question": "How can AI be used in spam email detection?",
    "answer": "AI, particularly through NLP techniques, analyzes the content of emails to determine if they are spam. It does this by identifying patterns, keywords, and stylistic choices that are common in spam messages. Here's a simplified breakdown of the process:\nSteps in AI-Based Spam Detection\n- Data Collection: \nA large dataset of emails, both spam and legitimate (\"ham\"), is gathered.\n- Preprocessing:\nEmails are cleaned and standardized. \nThis often involves:\nRemoving punctuation and special characters.\nConverting all text to lowercase.\nTokenization: Breaking down sentences into individual words.\nRemoving stop words (common words like 'the', 'and', etc.).\nStemming/Lemmatization: Reducing words to their base form (e.g., \"running\" becomes \"run\").\n- Feature Extraction:\nKey characteristics are extracted from the preprocessed emails to help identify spam. These might include:\nWord frequency: How often specific words appear.\n Let's go through an example to understand how Bag of Words (BoW) are used as input features for a machine learning model in spam email detection.\nExample Scenario:\n Suppose we have a small dataset of three emails:\n 1. Email 1: \"Buy cheap watches now\"\n 2. Email 2: \"Exclusive watches on sale\"\n 3. Email 3: \"Meeting tomorrow, please prepare the slides\"\nBag of Words creates a feature vector that counts how often each word appears in each email. Let's build a vocabulary of all unique words from our emails:\n - Vocabulary: [\"buy\", \"cheap\", \"watches\", \"now\", \"exclusive\", \"on\", \"sale\", \"meeting\", \"tomorrow\", \"please\", \"prepare\", \"the\", \"slides\"]\n Each email is represented as a vector of word counts:\n - Email 1: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n - \"buy\": 1, \"cheap\": 1, \"watches\": 1, \"now\": 1, all other words: 0\n - Email 2: [0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n - \"exclusive\": 1, \"watches\": 1, \"on\": 1, \"sale\": 1, all other words: 0\n - Email 3: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n - \"meeting\": 1, \"tomorrow\": 1, \"please\": 1, \"prepare\": 1, \"the\": 1, \"slides\": 1\n These vectors (lists of numbers) become the input features for the machine learning model. The model learns patterns based on word presence and frequency to distinguish between spam and non-spam emails.\nN-grams: Groups of consecutive words (e.g., \"free offer\").\n To understand how the n-grams input vector looks, let's use same example and see how we can represent emails using n-grams.\n Suppose we have the following emails:\n 1. Email 1: \"Buy cheap watches now\"\n 2. Email 2: \"Exclusive watches on sale\"\n 3. Email 3: \"Meeting tomorrow, please prepare the slides\"\n We will use bigrams (2-grams) for this example.\n 1. Generate Bigrams:\n - For Email 1: \"Buy cheap\", \"cheap watches\", \"watches now\"\n - For Email 2: \"Exclusive watches\", \"watches on\", \"on sale\"\n - For Email 3: \"Meeting tomorrow\", \"tomorrow please\", \"please prepare\", \"prepare the\", \"the slides\"\n 2. Build the Vocabulary:\n - List all unique bigrams from the emails:\n - [\"Buy cheap\", \"cheap watches\", \"watches now\", \"Exclusive watches\", \"watches on\", \"on sale\", \"Meeting tomorrow\", \"tomorrow please\", \"please prepare\", \"prepare the\", \"the slides\"]\n 3. Create the Input Vectors:\n - Each email will be represented as a vector where each element corresponds to the count of a bigram in the email, based on the vocabulary.\nInput Vectors for Each Email\n Let's create the input vector for each email using the bigrams vocabulary:\n - Email 1: \"Buy cheap watches now\"\n - \"Buy cheap\": 1\n - \"cheap watches\": 1\n - \"watches now\": 1\n - All other bigrams: 0\nEmail 1 vector: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n - Email 2: \"Exclusive watches on sale\"\n - \"Exclusive watches\": 1\n - \"watches on\": 1\n - \"on sale\": 1\n - All other bigrams: 0\nEmail 2 vector: [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n - Email 3: \"Meeting tomorrow, please prepare the slides\"\n - \"Meeting tomorrow\": 1\n - \"tomorrow please\": 1\n - \"please prepare\": 1\n - \"prepare the\": 1\n - \"the slides\": 1\n - All other bigrams: 0\nEmail 3 vector: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n These vectors are then used as input to a machine learning model to learn patterns and predict whether new emails are spam or not, based on the bigrams present in them.\nEmail metadata: Sender's address, subject line, etc.\nThis can be a mix of numeric and categorical data:\nNumeric: Length of subject line, number of recipients, etc.\nCategorical: Sender's domain, top-level domain (TLD) of URLs in the email, presence of specific keywords in the subject, etc. Categorical data would need to be encoded into numerical format (e.g., one-hot encoding, label encoding).\nSentiment analysis: Determining the emotional tone of the email.\n- Model Training:\nA machine learning model (e.g., Naive Bayes, Support Vector Machine, or even deep learning models) is trained on the labeled dataset. The model learns to distinguish between spam and ham based on the extracted features.\n \n \n- Model Evaluation: Test the model on a separate set of emails to see how well it distinguishes between spam and non-spam emails. Metrics like accuracy, precision, recall, and F1-score are used to evaluate performance.\n- Prediction:\nNew incoming emails are preprocessed and their features are extracted.\nThe trained model analyzes these features and predicts whether the email is spam or not.\n- Filtering:\nEmails classified as spam are automatically moved to the spam folder or blocked, while legitimate emails are delivered to the inbox.\n \n- Continuous Improvement: Regularly update the model with new data and retrain it to improve its accuracy and adapt to new spam tactics."
  },
  {
    "question": "How to build sentiment analysis model from scratch?",
    "answer": "- Data Gathering:\nCollect a set of text data (e.g., movie reviews, tweets, product feedback) where each piece of text is labeled with its corresponding sentiment (positive, negative, or neutral).\n- Text Preprocessing:\nClean and standardize the text:\nRemove punctuation, special characters, and HTML tags.\nConvert all text to lowercase.\nRemove stop words (common words like 'the', 'and', etc.)\nTokenize the text into individual words.\nConsider stemming or lemmatization to reduce words to their base forms (e.g., \"running\" becomes \"run\").\n- Feature Representation:\nConvert the preprocessed text into numerical features that a machine learning model can understand. Common approaches include:\nBag-of-Words (BoW): Create a dictionary of all unique words in your dataset. Represent each text as a vector where each element corresponds to the count of a specific word in that text.\nTF-IDF: Similar to BoW but weighs words based on their frequency in the text and their rarity across the entire dataset.\n- Model Training:\nChoose a simple classification algorithm (e.g., Naive Bayes, Logistic Regression).\nSplit your dataset into training and testing sets.\nTrain your chosen model on the training set, where it learns to associate the text features with their corresponding sentiment labels.\n- Model Evaluation:\nUse the trained model to predict the sentiment of the text in the testing set.\nCompare the model's predictions to the actual labels and calculate evaluation metrics like accuracy, precision, recall, and F1-score.\n- Prediction on New Data:\nOnce your model is trained and evaluated, you can use it to predict the sentiment of new, unseen text.\nNote:\nThis is a very simplified explanation of building a basic sentiment analysis model. In practice, more sophisticated techniques like deep learning models (e.g., Recurrent Neural Networks, Transformers) and word embeddings (e.g., Word2Vec, GloVe) can be used to achieve better performance."
  },
  {
    "question": "When to use tokenization and stemming/Lemmatization?",
    "answer": "Tokenization: \nAlways use tokenization as the first step in any NLP task that involves analyzing the text at the word level. \nIt's fundamental for breaking down sentences into individual words or subwords, which is necessary for further processing and analysis.\nStemming/Lemmatization: \nUse these when you want to reduce words to their base or root forms. This can be helpful for:\nReducing dimensionality: Fewer unique words to deal with.\nImproving generalization: Models can learn patterns across different word forms (e.g., \"run\", \"running\", \"runs\" all become \"run\").\nApplications where exact word forms are less important: Sentiment analysis, topic modeling, information retrieval.\nCan They Be Used Together?\nYes, tokenization is typically followed by stemming/lemmatization. \nTypical NLP Pipeline:\n- Tokenization: Break text into words/subwords.\n- Stemming/Lemmatization (Optional): Reduce words to their base forms.\n- Other Preprocessing: Remove stop words, handle punctuation, etc.\n- Feature Extraction: Convert text into numerical features (e.g., BoW, TF-IDF).\n- Modeling: Train and evaluate machine learning models.\nChoosing Stemming vs. Lemmatization:\nStemming:\nFaster and computationally less expensive.\nCan produce non-words (e.g., \"studies\" -> \"studi\").\nSuitable when meaning preservation is less critical.\nLemmatization:\nSlower but produces valid words.\nRequires part-of-speech tagging to work accurately.\nBetter when preserving meaning is important.\nExample:\nOriginal sentence: \"The cats were running and playing in the garden.\"\nTokenization: [\"The\", \"cats\", \"were\", \"running\", \"and\", \"playing\", \"in\", \"the\", \"garden\", \".\"]\nStemming: [\"the\", \"cat\", \"were\", \"run\", \"and\", \"play\", \"in\", \"the\", \"garden\", \".\"]\nLemmatization: [\"the\", \"cat\", \"be\", \"run\", \"and\", \"play\", \"in\", \"the\", \"garden\", \".\"]"
  },
  {
    "question": "What are the advantages and disadvantages of neural networks?",
    "answer": "Here are some advantages of Neural Networks\nStoring information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning. \nThe ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information. \nIt has fault tolerance: Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. \nHaving a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be shown to the network in all its aspects, the network can produce incorrect output \nGradual corruption: A network slows over time and undergoes relative degradation. The network problem does not immediately corrode.\nAbility to train machine: Artificial neural networks learn events and make decisions by commenting on similar events. \nParallel processing ability: Artificial neural networks have numerical strength that can perform more than one job at the same time. \nDisadvantages of Neural Networks\nHardware dependence: Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent. \nUnexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network. \nAssurance of proper network structure: There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. \nThe difficulty of showing the problem to the network: ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. \nThe duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results."
  },
  {
    "question": "What is the difference between bias and variance?",
    "answer": "Bias comes from model underfitting some set of data, whereas variance is the result of model overfitting some set of data.\nUnderfitting models have high error in training as well as test set. This behavior is called as ‘High Bias’\nConsider below example of bias(underfitting) where we are trying to fit linear function for nonlinear data.\nOverfitting models have low error in training set but high error in test set. This behavior is called as ‘High Variance’\nConsider below example of variance(overfitting) where complicated function creates lots of unnecessary curves and angles that are not related with data.\nLow bias (low underfitting) ML algorithms: Decision Tree, k-NN, SVM\nHigh bias (high underfitting) ML algorithms: Linear regression, Logistic regression\nHigh Variance (high overfitting) ML algorithms: Polynimial regression"
  },
  {
    "question": "What is bias-variance tradeoff",
    "answer": "As we increase the complexity of the model, error will reduce due to lower bias in the model. However, this will happen until a particular point. If we continue to make our model complex then model will overfit and lead to high variance.\nThe goal of any supervised ML algorithm to have low bias and low variance to achieve good prediction performance. This is \nWe can also use hyperparamters to adjust model complexity, few examples are as below\nThe K-NN algorithm has low bias(underfitting) and high variance(overfitting), tradeoff can be achieved by increasing the value of 'K'. \nHigher the value of 'K' means higher the number of neighbours, which in turn increases the bias of the model.\nThe SVM algorithm has low bias(underfitting) and high variance(overfitting), trade off can be achived by changing the 'C' paramter.\nThe C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. \nFor large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. \nConversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. \nFor very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.\nThe decision trees has low bias(underfitting) and high variance(overfitting), bias-variance tradeoff can be achived by changing the tree depth.\nIf the tree is shallow then we're not checking a lot of conditions/constrains i.e. the logic is simple or less complex, hence it automatically reduces over-fitting. This introduces more bias compared to deeper trees where we overfit the data. It can be imagined as we're deliberately not calculating more conditions means we're making some assumption (introduces bias) while creating the tree.\nThe linear regression has low variance(overfitting) and high bias(underfitting), bias-variance tradeoff can be acheived by increasing the number of features or by using another regression technique that can fit data better.\nIf data is not linearly separable then linear regression algorithm will result in low variance and high bias."
  },
  {
    "question": "What is more important model accuracy or model performance?",
    "answer": "Short answer is: Model accuracy matters the most! inaccurate information is not usefull.\nModel performance can be improved by increasing the compute resources.\nModel accuracy and performance can be subjective to the problem in hand. For example, in analysis of medical images to determine if there is a disease (such as cancer), the accuracy extremely critical, even if the models would take minutes or hours to make a prediction.\nSome applications require real time performance, even if this comes at a cost of accuracy. For example, imagine a machine that views a fast conveyor belt carrying tomatoes, where it must separate the green from the red ones. Though an occasional error is undesired, the success of this machine is more determined by its ability to withstand its throughput.\nA more common example is face detection for recreational applications. People would expect a fast response from the app, though the occasional missed face would not render it useless."
  },
  {
    "question": "What is the difference between machine learning and deep learning?",
    "answer": "Deep Learning out performs traditional ML techniques if the data size is large. But with small data size, traditional Machine Learning algorithms are p\n\n|Machine Learning|Deep Learning|\n\n\n|:-|:-|\n\n\n| Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned|Deep learning structures algorithms in layers to create an \"artificial neural network” that can learn and make intelligent decisions on its own|\n\n\n|Using handcrafted rules and feature engineering, ML algorithms can work well with small data. But its performance plateau once data increases.|Deep Learning algorithms need large data to understand it perfectly. Deep learning performances increases as data increases.|\n\n\n|Traditional ML algorithms can work on less computing power.|DL algorithms need high compute. There also special purpose compute for DL like GPu and TPU.|\n\n\n|In case of ML domain experts/Data scientists needs to do feature engineering in order to enable model o learn all data patterns.|One advantage with DL that it learns high level features from data.No extrnal feature engineering is required.|\n\n\n|ML models take less time to train|DL models take more time to train|\n\n\n| Ml models are easy to interpret as comare to DL models.|DL models are black box and its very difficult to interpret the results.|"
  },
  {
    "question": "Explain standard deviation and variance",
    "answer": "Variance:\nIn a Nutshell: Variance measures how spread out a set of data is. A high variance indicates that the data points are widely scattered, while a low variance means they are clustered close together.\nExample: Imagine two groups of students taking a test. \nGroup A: Scores are 60, 70, 75, 80, 90 \nGroup B: Scores are 40, 70, 70, 70, 100\nGroup B has a higher variance because its scores are more spread out from the average (70) compared to Group A.\nStandard Deviation:\nIn a Nutshell: Standard deviation is simply the square root of the variance. It's a more interpretable measure of spread because it's in the same units as the original data.\nExample: Continuing with the student test scores:\nIf the variance for Group A is 150, then its standard deviation is √150 ≈ 12.25. \nThis means that, on average, the scores in Group A deviate from the mean by about 12.25 points.\nWhy They Matter:\nUnderstanding Data Distribution: Variance and standard deviation help you understand how much your data varies from the average. This is crucial in many fields like finance (risk assessment), manufacturing (quality control), and social sciences (analyzing survey results).\nComparing Data Sets: These measures allow you to compare the spread of different datasets. For instance, you can see if one group of students has more varied test scores than another.\nStatistical Analysis: Variance and standard deviation are fundamental in many statistical tests and models. They help determine if differences between groups are significant or just due to chance.\nIn Simpler Terms:\nVariance: Think of it as the \"average squared distance\" of each data point from the mean.\nStandard Deviation: It's like the \"typical\" or \"average\" distance of data points from the mean.\nRemember, a higher variance or standard deviation means the data is more spread out, while a lower value indicates the data is more tightly clustered around the average. \nWe can expect about 68% of values to be within plus-or-minus 1 standard deviation."
  },
  {
    "question": "Explain confusion matrix",
    "answer": "What is a Confusion Matrix?\nImagine a table that helps you see how well your machine learning model is performing, especially when it comes to classification tasks (like deciding if an email is spam or not).\nIt compares the model's predictions to the actual, true labels of your data.\nWhy is it useful?\nBeyond Accuracy: A confusion matrix gives you a more detailed look at your model's performance than just overall accuracy.\nSpotting Weaknesses: You can see which classes your model struggles with.\nChoosing the Right Metric: Depending on your problem, you might care more about minimizing false positives or false negatives. The confusion matrix helps you calculate metrics like precision, recall, and F1-score which focus on these aspects.\nIn essence:\nA confusion matrix is like a scorecard for your classification model, showing you not just how many it got right, but also the specific types of errors it's making. This helps you understand its strengths and weaknesses, and make informed decisions about how to improve it.\nFrom our confusion matrix, we can calculate five different metrics measuring the validity of our model.\nACCURACY\n \n Accuracy is the ratio of correctly identified subjects in a pool of subjects.\n Accuracy = (all correct / all) = (TP+TN)/(TP+FP+FN+TN)\n Accuracy answers the question: How many patients did we correctly identify out of all patients?\nPRECISION\n \n Precision is the ratio of correctly identified +ve subjects by test, against all +ve subjects identified by test.\n Precision = (true positives / predicted positives) = TP/(TP+FP)\n Precision answers the question: How many patients tested +ve are actually +ve?\n This metric is often used in cases where classification of true positives is a priority. For example, a spam email classifier would rather classify some spam emails as regular emails rather than classify some regular emails as spam. That’s why some spam emails end up in your main inbox, just to be safe. (Here true positives are the spam emails)\nSENSITIVITY (RECALL)\n \n Sensitivity is the ratio of correctly identified +ve subjects by test against all +ve subjects in reality.\n Sensitivity = (true positives / all actual positives)= TP/(TP+FN)\n Sensitivity answers the question: Of all the patients that are +ve, how many did the test correctly predict?\n This metric is often used in cases where classification of false negatives is a priority. A good example is the medical test that we used for illustration above. The government would rather have some healthy people labeled +ve than have an infected individual labeled -ve and spread the disease. We would rather be overly cautious and have false positives than risk wrongly identifying false negatives.\nSPECIFICITY\n \n Specificity is the ratio of correctly identified -ve subjects by test against all -ve subjects in reality.\n Specificity = (true negatives / all actual negatives) = TN/(TN+FP)\n Specificity answers the question: Of all the patients that are -ve, how many did the test correctly predict?\n This metric is often used in cases where classification of true negatives is a priority. For example, a doping test will immediately ban an athlete if they are tested positive. We would not want to any drug-free athlete to be wrongly classified and banned.\nF1 SCORE\n \n F1 Score accounts for both precision and sensitivity.\n F1 Score = 2 (Recall Precision)/(Recall + Precision)\n It is often considered a better indicator of a classifier’s performance than a regular accuracy measure as it compensates for uneven class distribution in the training dataset. For example, an uneven class distribution is likely to occur in insurance fraud detection, where a large majority of claims are legitimate and only a very small minority are fraudulent. \nWhich metric to use is depends on the problem in hand"
  },
  {
    "question": "Why do we need confusion matrix?",
    "answer": "We can not rely on a single value of accuracy in classification when the classes are imbalanced. \nFor example, we have a dataset of 100 patients in which 5 have diabetes and 95 are healthy. However, if our model only predicts the majority class i.e. all 100 people are healthy then also we will have a classification accuracy of 95%.\nConfusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. \nConfusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives."
  },
  {
    "question": "Explain collinearity and technique to reduce it?",
    "answer": "In statistics collinearity or multicollinearity is the phenomenon where one or more predictive variables(features) in multiple regression models are highly linearly related to each other.\nTechnique to reduce multicollearity\nRemove highly correlated predictors from the model. If you have two or more factors with a high collinearity, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared. Consider using stepwise regression, best subsets regression, or specialized knowledge of the data set to remove these variables. Select the model that has the highest R-squared value. \nPrincipal Components Analysis(PCA) regression methods that cut the number of predictors to a smaller set of uncorrelated components."
  },
  {
    "question": "Difference between statistics and machine learning",
    "answer": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.\nStatistics is mathematical study of data. Lots of statistical models that can make predictions, but predictive accuracy is not their strength."
  },
  {
    "question": "In a test, students in section A scored with a mean of 75 and standard deviation of 10, while students in section B scored with a mean of 80 and standard deviation of 12? Melissa from section A and Ryan from section B both have scored 90 in this test. Who had a better performance in this test as compared to their classmates?",
    "answer": "To compare the two scores we need to standardize them to the same scale. We do that by calculating the Z score, which allows us to compare the 2 scores in units of standard deviations. \nZ score= (X- mean)/Standard Deviation\nMelissa's Z score = (90-75)/10 = 1.5\nRyan's Z score = (90-80)/12 = 0.83\nMelissa has performed better."
  },
  {
    "question": "What is null hypothesis and alternate hypothesis?",
    "answer": "The null hypothesis states that a population parameter (such as the mean, the standard deviation, and so on) is equal to a hypothesized value. The null hypothesis is often an initial claim that is based on previous analyses or specialized knowledge.\nThe alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis. The alternative hypothesis is what you might believe to be true or hope to prove true.\nSo when running a hypothesis test/experiment, the null hypothesis says that there is no difference or no change between the two tests. The alternate hypothesis is the opposite of the null hypothesis and states that there is a difference between the two tests."
  },
  {
    "question": "What is a hypothesis test and p-value?",
    "answer": "A hypothesis test examines two opposing hypotheses about a population: the null hypothesis and the alternative hypothesis. The null hypothesis is the statement being tested. Usually the null hypothesis is a statement of \"no effect\" or \"no difference\". The alternative hypothesis is the statement you want to be able to conclude is true based on evidence provided by the sample data.\nBased on the sample data, the test determines whether to reject the null hypothesis. You use a p-value, to make the determination. If the p-value is less than the significance level (denoted as alpha), then you can reject the null hypothesis.\nIn laymans term the p-value is the probability that the null hypothesis is true.\nConsider the example where we are trying to test whether a new marketing campaign generates more revenue\nHere null hypothesis states that there is no change in the revenue as a result of the new marketing campaign\nBased on p-value we can accept or reject the null hypothesis. 0.25 p-value means there is 25% chance that new marketing campaign will not change revenue\nLower the p-value, the more confident we are that the alternate hypothesis is true, which, in this case, means that the new marketing campaign causes an increase or decrease in revenue.\nIn most fields, acceptable p-values should be under 0.05 while in other fields a p-value of under 0.01 is required.\nSo when a result has a p-value of 0.05 or lower we can reject null hypothesis and accept the alternate hypothesis. \nMore Info: Basic Concepts of Hypothesis Testing\nIn simple term hypothesis is a assumption. Since its assumption, after our testing it can hold true may not.\nIf our assumption holds true after testing then it is termed as 'Null Hypothesis' unless there is evidence against it.\nIf our assumption dont hold true and there is claim aganist it, then it is termed as alternate hypothesis.\nSo when our assumption dont hold true then Type I error occurs. Here we are going against the Null Hypothesis.\np-value: It is calculated probability of making type I error"
  },
  {
    "question": "What is power of hypothesis test? Why is it important?",
    "answer": "Remember that if actual value is positive and our model predicts it as negative then Type II error occuras (False negative). e.g. Calling a guilty person innocent, diaognosing cancer infected person as healthy etc. \nThe probability of not commiting Type II error is called as power of hypothesis test. The higher probability we have of not commiting a type 2 error, the better our hypothesis test is."
  },
  {
    "question": "What is the difference betweeen K nearest neighbors and K means",
    "answer": "KNN or K nearest neighbor is a classification algorithm, while K-Means is clustering technique.\nKNN is supervised algorithm, K means is unsupervised algorithm.\nIn KNN prediction of the test sample is based on the similarity of its features to its neighbors. The similarity is computed based on the measure such as euclidean distance. Here K \nK-means is the process of defining clusters or groups around predefined centroids based on the similarity of each data point to each other. Here K"
  },
  {
    "question": "Explain Random forest algorithm",
    "answer": "Random forest is supervised learning algorithm and can be used to solve classification and regression problems. \nSince decision-tree create only one tree to fit the dataset, it may cause overfitting and model may not generalize well. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. \nAveraging the results from multiple decision trees help to control the overfitting and results in much better prediction accuracy. As you may have noticed, since this algorithm uses multiple trees hence the name ‘Random Forest’\nThis algorithm is heavily used in various industries such as Banking and e-commerce to predict behavior and outcomes."
  },
  {
    "question": "Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?",
    "answer": "Yes, Random Forest can be used for both continuous and categorical target (dependent) variables.\nIn a random forest the classification model"
  },
  {
    "question": "What do you mean by Bagging?",
    "answer": "In bagging we build independent estimators on different samples of the original data set and average or vote across all the predictions.\nBagging is a short form of Bootstrap Aggregating. It is an ensemble learning approach used to improve the stability and accuracy of machine learning algorithms.\nSince multiple model predictions are averaged together to form the final predictions, Bagging reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.\nBagging is a special case of the model averaging approach, in case of regression problem we take mean of the output and in case of classification we take the majority vote.\nBagging is more helpfull if we have over fitting (high variance) base models.\nWe can also build independent estimators of same type on each subset. These independent estimators also enable us to parallelly process and increase the speed.\nMost popular bagging estimator is 'Bagging Tress' also knows as 'Random Forest'\nBootstrapping\nIt is a resampling technique, where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample.\nSo this technique will enable us to produce as many subsample as we required from the original training data.\nThe defination is simple to understand, but \"replacement\" word may be confusing sometimes. Here 'replacement' word signifies that the same obervation may repeat more than once in a given sample, and hence this technique is also known as sampleing with replacement\nAs you can see in above image we have training data with observations from X1 to X10. In first bootstrap training sample X6, X10 and X2 are repeated where as in second training sample X3, X4, X7 and X9 are repeated.\nBootstrap sampling helps us to generate random sample from given training data for each model in order to genralise the final estimation.\nSo in case of Bagging we create multiple number of bootstrap samples from given data to train our base models. Each sample will contain training and test data sets which are different from each other and remember that training sample may contain duplicate observations."
  },
  {
    "question": "What is Out-of-Bag Error in Random Forests?",
    "answer": "Out-of-Bag is equivalent to validation or test data but it is calculated internally by Random Forest algorithm. In case of Sklearn if we set hyperparameter 'oobscore = True' then Out-of-Bag score will be calculated for every decision tree.\nFinally, we aggregate all the errors from all the decision trees and we will determine the overall OOB error rate for the classification.\nFor more details"
  },
  {
    "question": "What is the use of proximity matrix in the random forest algorithm?",
    "answer": "A proximity matrix is used for the following cases :\nMissing value imputation\nDetection of outliers"
  },
  {
    "question": "List down the parameters used to fine-tune the Random Forest.",
    "answer": "Two parameters that have to fine-tune to improve the predictions that are important in the random forest algorithm are as follows:\nNumber of trees used in the forest (ntree)\nNumber of random variables used in each of the trees in the forest (mtry)"
  },
  {
    "question": "What is K Fold cross validation? Why do you use it?",
    "answer": "In case of K Fold cross validation input data is divided into ‘K’ number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method.\nThis significantly reduces underfitting as we are using most of the data for training(fitting), and also significantly reduces overfitting as most of the data is also being used in validation set.\nK Fold cross validation helps to generalize the machine learning model, which results in better predictions on unknown data."
  },
  {
    "question": "How to handle missing data?",
    "answer": "Data can be missing because of mannual error or can be gennualy missing.\nDelete low quality records completely which have too much missing data\nImpute the values by educated guess, taking average or regression\nUse domain knwledge to impute values"
  },
  {
    "question": "What is the difference between Bar graph and histogram?",
    "answer": "Bar graph is used for descreate data where as histogram is used for continuous data.\nIn bar graph there is space between the bars and in case of histogram there is no space between the bars(contnuous scale).\nIn bar graph the order of the bars can be changed and in histogram order remains same."
  },
  {
    "question": "What is the Box and Whisker plot? When should use it?",
    "answer": "Box and whisker plots are ideal for comparing distributions because the centre, spread and overall range are immediately apparent. \nA box and whisker plot is a way of summarizing a set of data measured on an interval scale. \nIt is often used in explanatory data analysis\nBoxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”).\nmedian (Q2/50th Percentile): the middle value of the dataset.\nfirst quartile (Q1/25th Percentile): the middle number between the smallest number (not the “minimum”) and the median of the dataset.\nthird quartile (Q3/75th Percentile): the middle value between the median and the highest value (not the “maximum”) of the dataset.\ninterquartile range (IQR): 25th to the 75th percentile.\nwhiskers (shown in blue)\noutliers (shown as green circles)\n“maximum”: Q3 + 1.5IQR\n“minimum”: Q1 -1.5IQR"
  },
  {
    "question": "What is outlier? How to handle them?",
    "answer": "An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.\nData points above and below 1.5IQR, are most commonly outliers.\nOutliers can drastically change the results of the data analysis and statistical modeling.\nTypes of the outliers\nData entry errors\nMeasuremental errors\nIntentional outliers. This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume.\nData processing erros. Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\nSampling error. For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\nNatutal oulier. When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my problem assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.\nHow to detect Outliers?\nMost commonly used method to detect outliers is visualization.\nWe use various visualization methods, like Box-plot, Histogram, Scatter Plot \nUse capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\nData points, three or more standard deviation away from mean are considered outlier\nApart from visualization we can also use Z-Score or Extreme Value Analysis (parametric) to detect outliers.\nHow to remove outliers?\nMost of the methods used to handle missing values are aslo application in case ot outliers\nDeleting observations\nWe delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\nTransforming and binning values\nTransforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable.\nImputing\nWe can use mean, median, mode imputation methods.\nTreat separately\nIf there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output."
  },
  {
    "question": "If deleting outliers is not an option, how will you handle them?",
    "answer": "I will try differen models. Data detected as outliers by linear model, can be fit by non-linear model.\nTry normalizing the data, this way the extreame datapoints are pulled to the similar range.\nWe can use algorithms which are less affected by outliers.\nWe can also create separate model to handle the outlier data points."
  },
  {
    "question": "You fit two linear models on a dataset. Model 1 has 25 predictors and model 2 has 10 predictors. What performance metric would you use to select the best model based on training dataset?",
    "answer": "First of all model performace is not directly proportional to the number of predictors, so we cant say that model with 25 predictors is better than the model with 10 predictors\nHere important thing is to understand different evaluation metric for linear regresion and which one of them can help us identify the impact of number of predictors on model performance.\nEvaluation metric used for linear regression are MSE, MAE, R-squared, Adjusted R-squared, and RMSE.\nMSE penalizes large errors, MAE does not penalize large errors, RMSE penalizes large errors and R-squared or Coefficient of Determination represent the strength of the relationship between your model and the dependent variable.\nThough R-squared represent the strength of relationship between model and the dependent variables, it is never used for comparing the models as the value of R² increases with the increase in the number of predictors (even if these predictors do not add any value to the model)\nNow only remaining metric is Adjusted R-squared. Unlike R-squared, Adjusted R-squared measures variation explained by only the independent variables that actually affect the dependent variable.\nSo the Adjusted R-squre score will increase only if addition of predictors improve the models performance significantly or else it will decrease. Hence correct answer is Adjusted R-squared"
  },
  {
    "question": "Suppose we have a function -4x^2 + 4x + 3. Find the maximum or minimum of this function.",
    "answer": "This is quadratic equation,\n f(x) = -4x^2 + 4x + 13 (for a function: ax^2 + bx + c, when a < 0, then function has maximum value)\nTo find the slope of the function, lets take derivative of it\n f'(x)= -8x + 4\nAt maximum point, slope will be 0\n -8x + 4 = 0\n \n x = 0.5\nNow lets put 0.5 in equation to find the maximum values\n f(0,5) = -4(0.5)^2 + 4(0.5) + 13 = -1 + 2 +13 = 14\nThis functiona will have concave shape. So the maximum point is (0.5, 14)"
  },
  {
    "question": "Below is the output of a correlation matrix from your Exploratory data. Is using all the features in a model appropriate for predicting/inferencing Y?",
    "answer": "We can see from above correlation matrix that there is high correlation(.98) between X1 and X2, also high correlation(.88) between X1 and X3, similarly there is high correlation(.75) between X2 and X3\nAll the variables are correlated to each other. In regression this would result in multicollinearity. We can try methods such as dimension reduction, feature selection, stepwise regression to choose the correct input variables for predictiong Y\nSecond part of question is - should we use all the variables for modeling?\n - Using multicolnear feature in modeling doesnt help. We should remove all the multicolnear feature and keep unique feature so that explaining the model predictions also becomes easy.\n - It will also make model less complex and we dont have to store many features."
  },
  {
    "question": "What is stepwise regression?",
    "answer": "Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion.\nStepwise regression is classified into backward and forward selection.\nBackward selection starts with a full model, then step by step we reduce the regressor variables and find the model with the least RSS, largest R², or the least MSE. The variables to drop would be the ones with high p-values.\nForward selection starts with a null model, then step by step we increase the regressor variables until we can no longer improve the error performance of the model. We usually pick the model with the highest adjusted R²."
  },
  {
    "question": "You have two buckets - one of 3 liters and other of 5 liters. You are expected to mesure exactly 4 liters. How will you complete the task? Note: There is no thrid bucket.",
    "answer": "Questions like this will test your out of the box thinking\nStep1: Fill 5 lts bucket and empty it in 3 ltr bucket. Now we are left with 2 ltr in 5 ltr bucket.\nStep2: Empty 3 ltr bucket and pour the contents of 5 ltr bucket in 3 ltr bucket. Now our 5 ltr bucket is empty and 3 ltr bucket has 2 ltr content in it.\nNow fill the 5 ltr bucket again. Remember that our 3 ltr bucket has 2 ltr content in it, so if we pour 1 ltr content from 5 ltr bucket to 3 ltr bucket we are left with 4 ltr content in 5 ltr bucket."
  },
  {
    "question": "Lis the differences between supervised and unsupervised learning",
    "answer": "| Supervised learning |\n Unsupervised leanring\n\n|:- |\n:-\nUses labeled data as input | Uses unlabeled data as input\nSupervised learning has feedback mechanism | Unsupervised learning has no feedback mechanism\nCommon supervised learning algorithms are decision tree, logistic regression, support vector machine etc | K Means clustering, hierarchical clustering etc"
  },
  {
    "question": "Explain the steps in making decision tree?",
    "answer": "Below are the common steps in decision tree algorithm\nTake the entire data as input\nAt the root node decision tree selects feature to split the data in two major categories.\nDifferent criteria will be used to split the data. We generally use 'entropy' or 'gini' in case of classification and 'mse' or 'mae' in case of regression problems.\nFeatures are selected for spliting based on highest information gain.\nAfter every split we get decision rules and sub trees.\nThis process will continue until every training example is grouped together or maxinum allowed tree depth is reached.\nSo at the end of decision tree we end up with leaf node. Which represent the class or a continuous value that we are trying predict"
  },
  {
    "question": "How do you build random forest model?",
    "answer": "Ranodm forest is made up of multiple decision trees. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. \nSelect few random sub sample from given dataset\nConstruct a decision tree for every sub sample and predict the result.\nPerform the voting on prediction from each tree.\nAt the end select the most voted result as final prediction."
  },
  {
    "question": "How do Random Forest handle missing data?",
    "answer": "Random Forests inherently have two primary ways of handling missing data:\n- During Training (Building the Trees):\nFor Numerical Features: Missing values can be imputed using simple strategies like mean or median.\nFor Categorical Features: A new \"missing\" category is often created to handle missing values. This ensures that data points with missing categorical values are still considered during the tree building process.\n- During Prediction (Making New Predictions):\n\"Surrogate\" Splits: Each tree in the forest stores \"surrogate\" splits along with the primary split at each node. Surrogate splits are based on other features that are highly correlated with the primary split feature. If a new data point encounters a missing value during prediction, the tree will use the surrogate split to guide the data point down the appropriate branch.\nProximity Measures: Random Forests also calculate \"proximity measures\" between data points based on how often they end up in the same leaf nodes across all the trees. These proximities can be used to impute missing values by taking a weighted average of the values from similar data points.\nNeed to review below answer....\nNote that handling missing data is one of the advantages of Random Forest algorithm over Decision tree. Please \nRandom forest will create three sub sample of 9 training examples each\nRandom forest algorithm will create three different decision tree for each sub sample\nNotice that each tree uses different criteria to split the data\nNow it is straight forward analysis for the algorithm to predict the shape of given figure if its shape and color is known. Let’s check the predictions of each tree for blue color triangle, (here shape input is missing)\nTree 1 will predict: triangle\nTree 2 will predict: square\nTree 2 will predict: triangle \n \n Since the majority of voting is for triangle final prediction is ‘triangle shape’\nNow, lets check predictions for circle with no color defined (color attribute is missing here)\nTree 1 will predict: triangle\nTree 2 will predict: circle\nTree 2 will predict: circle \n \n Since the majority of voting is for circle final prediction is ‘circle shape’\nPlease note this is over simplified example, but you get an idea how multiple tree with different split criteria helps to handle missing features"
  },
  {
    "question": "What is model overfitting? How can you avoid it?",
    "answer": "Overfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. Below images represent the overfitting linear and logistic regression models.\nHow To Avoid Overfitting?\nSince overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for same\nWe can also use the ‘Regularization’ technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter ‘alpha’ to control the size of the coefficients by imposing a penalty. \nK-fold cross validation. In this technique we divide the training data in multiple batches and use each batch for training and testing the model.\nIncreasing the training data also helps to avoid overfitting."
  },
  {
    "question": "There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier ball?",
    "answer": "To find the heavier ball among 9 balls using a balance scale, you can determine the minimum number of weighings required by strategically dividing the balls and comparing their weights.\nStep-by-Step Solution:\n- First Weighing:\n - Divide the 9 balls into three groups of 3 balls each: Group A, Group B, and Group C.\n - Weigh Group A against Group B.\n- Analyzing the First Weighing:\n - Case 1: If the scales balance (i.e., Group A = Group B), it means the heavier ball is in Group C.\n - Case 2: If the scales do not balance (i.e., Group A ≠ Group B), the heavier ball is in the group that tips the scale.\n- Second Weighing:\n - You now have 3 balls (either all from Group C in Case 1, or from the heavier group in Case 2).\n - Take 2 of these 3 balls and weigh them against each other.\n- Analyzing the Second Weighing:\n - Case 1: If the scales balance, the heavier ball is the one that was not weighed.\n - Case 2: If the scales do not balance, the heavier ball is the one that tips the scale.\nConclusion:\nThe minimum number of weighings required to find the heavier ball among the 9 balls is 2.\nBy dividing the balls into three groups and strategically using the balance scale, you can ensure that you find the heavier ball in just two weighings."
  },
  {
    "question": "Difference between univariate, bivariate and multivariate analysis?",
    "answer": "Univariate Analysis\nBivariate Analysis\nMultivariate Analysis"
  },
  {
    "question": "What are feature selection methods to select right variables?",
    "answer": "Feature selection is the process of reducing the number of input variables when developing a predictive model. There are two methods for feature selection. Filter method and wrapper methods. Best analogy for selecting features is bad data in bad answers out.\nFilter Methods\nFilter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.\nThese methods are faster and less computationally expensive than wrapper methods.\nInformation Gain\nInformation gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.\nChi-square Test\nThe Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. \nCorrelation Coefficient\nCorrelation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the good variables are highly correlated with the target. Furthermore, variables should be correlated with the target but should be uncorrelated among themselves.\nWrapper Methods\nWrapper feature selection methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric.\nThese methods are unconcerned with the variable types, although they can be computationally expensive.\nThe wrapper methods usually result in better predictive accuracy than filter methods.\nForward Feature Selection\nThis is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved.\nBackward Feature Elimination\nThis method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model. Next, we remove the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved.\nExhaustive Feature Selection\nThis is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset."
  },
  {
    "question": "You are given a dataset consisting of variables having more than 30% missing values? How will you deal with them?",
    "answer": "There are multiple ways to handle missing values in the data\nIf dataset is huge we can simply remove the rows containing the missing data\nIf dataset is small then we have to impute the missing values. There are multiple ways to impute the missing values. In case of categorical data we may use the most common values and in case numerical data we can use mean, median etc."
  },
  {
    "question": "For the given point how will you caluclate the Euclidean distance, in Python?",
    "answer": "Euclidean distance is calculated as the square root of the sum of the squared differences between the two vectors."
  },
  {
    "question": "How should you maintain your deployed model?",
    "answer": "Monitor\nConstant monitoring of all the models is needed to determine the performance accuracy of the models\nEvaluate\nEvaluation metric of the current model is calculated to determine if new algorithm is needed.\nCompare\nThe new models are compared against each other to determine which model performs the best.\nRebuild\nThe best performing model is re-built on the current set of data."
  },
  {
    "question": "What are recommender systems?",
    "answer": "The purpose of a recommender system is to suggest relevant items or services to users.\nTwo major categories of recommender systems are collaboarative filtering and cotent based filtering methods\nCollaborative Filtering\nIt is based on the past interactions recorded between users and items in order to produce new recommendations.\ne.g. Music service recommends track that are often played by other users with similar interests\nContent Based Filtering\nUnlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about the content consumed by the user to produce new recommendations\ne.g. Music service recommends new song based on properties of the songs user listens to."
  },
  {
    "question": "'People who bought this, also bought...'recommendations seen on Amazon is a result of which algorithm?",
    "answer": "Its done by recommendation system using collaborative filtering approach.\nIn case of collaborative filtering past interactions recorded between users and items are used to produce new recommendations."
  },
  {
    "question": "If it rains on saturday with probability 0.6, and it rains on sunday with probability 0.2, what is the probability that it rains this weekend?",
    "answer": "Since we know the probability of rain on Saturday and Sunday, the probability of raining on Weekend is combination of both of these events.\nTrick here is to know the probability of not raining on Saturday and Sunday.\nIf we subtract the intersection(∩) of both the events of not raining on Saturday and Sunday from total probability then we get the probability of raining on weekend.\n \n = Total probability - (Probability that it will not rain on Saturday) ∩ (Probability that it will not rain on Sunday)\n = 1 - (1 - 0.6)(1 - 0.2)\n = 0.68"
  },
  {
    "question": "How can you select K for K-Means?",
    "answer": "There are two ways to select the number of clusters in case K-Means clustering algorithm\nVisualization\nTo find the number of clusters manually by data visualization is one of the most common method. \nDomain knowledge and proper understanding of given data also help to make more informed decisions. \nSince its manual exercise there is always a scope for ambiguous observations, in such cases we can also use ‘Elbow Method’\nElbow Method\nIn Elbow method we run the K-Means algorithm multiple times over a loop, with an increasing number of cluster choice(say from 1 to 10) and then plotting a clustering score as a function of the number of clusters. \nClustering score is nothing but sum of squared distances of samples to their closest cluster center. \nElbow is the point on the plot where clustering score (distortion) slows down, and the value of cluster at that point gives us the optimum number of clusters to have. \nBut sometimes we don’t get clear elbow point on the plot, in such cases its very hard to finalize the number of clusters."
  },
  {
    "question": "Explain dimensionality reduction, and its benefits?",
    "answer": "Dimensionality reduction \nIt helps in data compressing and reducing the storage space\nIt reduces computation time as less dimensions lead to less computing\nIt removes redundant features. E.g. There is no point in storing value in two different units"
  },
  {
    "question": "How can you say that the time series data is stationary?",
    "answer": "For accurate analysis and forecasting, trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time."
  },
  {
    "question": "How can you calculate the accuracy using confusion matrix?",
    "answer": "Accuracy = (True Positive + true Negative) / Total Obervations"
  },
  {
    "question": "Write the equations for the precision and recall?",
    "answer": "Precision = True Positive / (True Positive + False Positive)\nRecall = True Positive /(Total Positive + False Negative)"
  },
  {
    "question": "If a drawer containes 12 red socks, 16 blue socks, and 20 white socks, how many must you pull out to be sure of having a amcthing pair?",
    "answer": "There are three colors of socks- Red, Blue and White. No of socks is irrelevant here.\nSuppose in our first pull we picked Red color sock\nIn second pull we picked Blue color sock\nAnd in third pull we picked White color sock.\nNow in our fourth pull, if we pick any color, match is guaranteed!! So the answer is 4!"
  },
  {
    "question": "Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuos variables?",
    "answer": "- K-means clustering\n- Linear regression\n- K-NN\n- Decision tress\nUsing KNN we can compute the missing variable value by using the nearest neighbors."
  },
  {
    "question": "Given a box of matches and two ropes, not necessarily identical, measure a period of 45 minutes? Note: Ropes are not uniform in natire and rope takes exactly 60 minutes to completly burn out",
    "answer": "We have two ropes A and B\nLigt A from both the end and B from one end\nWhen A finished burning we know that 30 minutes have elapsed and B has 30 minutes remaining\nNow light the other end of B also, it will now burnout in 15 minutes\nThis we got 30 + 15 = 45 minutes"
  },
  {
    "question": "After studying the behaviour of population, you have identified four specific individual types who are valueable to your study. You would like find all users who are most similar to each indivdual type. Which algorithm is most approprate for this study?",
    "answer": "The most appropriate algorithm for this study is K-Nearest Neighbors (KNN). \nHere's why KNN is well-suited for this task:\n- Similarity-Based: KNN explicitly focuses on finding the most similar data points (users in this case) to a given point (your identified individual types) based on their features or attributes. \n- No Assumption about Data Distribution: KNN is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying distribution of your data. This is beneficial when you're unsure how the data is distributed or if it doesn't fit a specific statistical model.\n- Easy to Interpret: KNN is relatively easy to understand and interpret. You simply define the number of neighbors (k) and a distance metric (e.g., Euclidean distance), and the algorithm finds the k closest data points to each of your individual types.\n- Flexible: KNN can handle both numerical and categorical data, making it adaptable to different types of features that might be relevant in your study."
  },
  {
    "question": "Your organization has a website where visitors randomly receive one of the two coupons. It is also possible that visitors to the website will not receive the coupon. You have been asked to determine if offering a coupon to the visitors to your website has any impact on their purchase decision. Which analysis method should you use?",
    "answer": "In this scenario, the most appropriate analysis method would be A/B Testing (or Split Testing). \nHere's why A/B Testing is the best fit:\nControlled Experiment: A/B Testing allows you to randomly assign visitors to different groups (control group with no coupon, group A with coupon type 1, group B with coupon type 2). This controlled experiment ensures that any differences in purchase behavior can be directly attributed to the presence and type of coupon. \nMeasures Impact: You can directly compare the conversion rates (percentage of visitors who make a purchase) between the groups to see if offering a coupon has a statistically significant impact on purchase decisions. \nCompares Coupon Types: Additionally, you can analyze if one coupon type is more effective than the other in driving purchases.\nHow to Implement A/B Testing:\n- Define the Hypothesis: State clearly what you want to test. For example:\nNull Hypothesis: Offering a coupon has no impact on the purchase decision.\nAlternative Hypothesis: Offering a coupon increases the likelihood of a purchase. \n- Set up the Experiment: \nRandomly assign website visitors to one of the three groups (control, coupon A, coupon B).\nEnsure the experiment runs for a sufficient duration to collect enough data for statistically significant results.\n- Collect Data: \nTrack the conversion rates for each group.\nGather additional data like coupon usage, time spent on site, etc., for further analysis.\n- Analyze the Results: \nUse statistical tests (e.g., Chi-squared test, t-test) to compare the conversion rates between the groups and determine if the differences are statistically significant.\nCalculate metrics like uplift (increase in conversion rate due to the coupon) and confidence intervals to understand the impact of the coupons.\n- Draw Conclusions and Take Action: \nBased on the analysis, decide if offering coupons is beneficial.\nIf so, identify the most effective coupon type and implement it in your marketing strategy.\nImportant Considerations:\nSample Size: Ensure you have a large enough sample size in each group to achieve statistically significant results.\nExternal Factors: Be mindful of external factors (e.g., seasonality, promotions by competitors) that could influence purchase behavior during the experiment.\nEthical Considerations: Be transparent with visitors about the experiment and obtain necessary consent if required.\nA/B Testing provides a robust and data-driven way to understand the impact of coupons on purchase decisions, enabling you to make informed marketing decisions and optimize your website's conversion rate."
  },
  {
    "question": "Explain Principal Componenet Analysis?",
    "answer": "Principal Component Analysis (PCA) is dimensionality reduction method, that is used to reduce dimensionality of large data sets, by transforming large set of variables into a smaller one that still contains most of the information in large set.\nPrincipal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the “new” variables after PCA are all independent of one another\nReducing the number of the variables of the datset naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity.\nBy reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model\nWhen should I use PCA?\nDo you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\nDo you want to ensure your variables are independent of one another?\nAre you comfortable making your independent variables less interpretable?"
  },
  {
    "question": "Explain feature scaling",
    "answer": "Feature scaling is one of the most important data preprocessing step in machine learning\nIf we are changing the range of the features then its called 'scaling' and if we are changing the distribution of the features then its called 'normalization/standardization'\nScaling\nThis means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. By scaling your variables, you can help compare different variables on equal footing.\nScaling is required in case of distance based algorithms like support vector machines (SVM) or k-nearest neighbors (KNN).\nFor example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies.\nBut what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter). For example, if weight ranges from 50 to 100 kilograms and height ranges from 150 to 200 centimeters, the model might give more importance to the weight feature simply because its range is wider. Without scaling, a model might consider a difference of 10 kilograms as equally significant as a difference of 10 centimeters, even though these units measure very different things. This could lead to misleading results because the model's decisions are based on the scale rather than the underlying relationships."
  },
  {
    "question": "Difference between standardisation and normalization?",
    "answer": "Standardization\nWhat it does:\nCenters the data around zero (mean = 0)\nScales the data to have a standard deviation of one (std = 1)\nTransformation: \nZ = (X - mean) / stddev\nWhen to use it:\nAlgorithms that are sensitive to the scale of features (e.g., linear regression, logistic regression, support vector machines).\nWhen you assume your data follows a normal (Gaussian) distribution (though not strictly required).\nWhen outliers are present, as standardization is less affected by them. ( It means it includes the effect of outliers in scaling process)\nNormalization\nWhat it does: \nScales the data to a specific range, typically between 0 and 1\nTransformation:\nXnorm = (X - Xmin) / (Xmax - Xmin)\nWhen to use it:\nAlgorithms that rely on distance calculations (e.g., k-nearest neighbors, k-means clustering).\nWhen you don't know the distribution of your data.\nWhen you want to avoid outliers having a disproportionate impact on the scaling.( It means it tries to limit the impact of outliers on scaling process)\nKey differences in a nutshell:\nTransformation: Standardization uses mean and standard deviation. Normalization uses minimum and maximum values.\nDistribution Assumption: Standardization often assumes a normal distribution. Normalization makes no assumptions about the distribution.\nOutlier Sensitivity: Standardization is less sensitive to outliers. Normalization is more sensitive to outliers.\nRange: Standardization has no fixed range. Normalization scales to a specific range (usually 0 to 1).\nChoosing the right one:\nNo one-size-fits-all answer: The best choice depends on your specific data, algorithm, and problem. \nExperimentation is key: Try both techniques and compare the results to see which one works better for your particular case."
  },
  {
    "question": "What is meant by Data Leakage?",
    "answer": "Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting.\nIn Machine learning, Data Leakage \nData leakage is a serious and widespread problem in data mining and machine learning which needs to be handled well to obtain a robust and generalized predictive model.\nExamples of data leakage\nThe most obvious and easy-to-understand cause of data leakage is to include the target variable as a feature. What happens is that after including the target variable as a feature, our purpose of prediction got destroyed. This is likely to be done by mistake but while modelling any ML model, you have to make sure that the target variable is differentiated from the set of features.\nAnother common cause of data leakage is to include test data with training data.\nAbove two cases are not very likely to occur because they can easily be spotted while doing the modelling. Below are few data leakage examples that are hard to troubleshoot.\nPresence of Giveaway features\nLet’s we are working on a problem statement in which we have to build a model that predicts a certain medical condition. If we have a feature that indicates whether a patient had a surgery related to that medical condition, then it causes data leakage and we should never be included that as a feature in the training data. The indication of surgery is highly predictive of the medical condition and would probably not be available in all cases. If we already know that a patient had a surgery related to a medical condition, then we may not even require a predictive model to start with.\nLet’s we are working on a problem statement in which we have to build a model that predicts if a user will stay on a website. Including features that expose the information about future visits will cause the problem of data leakage. So, we have to use only features about the current session because information about the future sessions is not generally available after we deployed our model.\nLeakage during Data preprocessing\nWhile solving a Machine learning problem statement, firstly we do the data cleaning and preprocessing which involves the following steps:\nEvaluating the parameters for normalizing or rescaling features\nFinding the minimum and maximum values of a particular feature\nNormalize the particular feature in our dataset\nRemoving the outliers\nFill or completely remove the missing data in our dataset\nThe above-described steps should be done using only the training set. If we use the entire dataset to perform these operations, data leakage may occur. \nApplying preprocessing techniques to the entire dataset will cause the model to learn not only the training set but also the test set. As we all know that the test set should be new and previously unseen for any model."
  },
  {
    "question": "How to detect Data Leakage?",
    "answer": "Results are too good too true\nIn general, if we see that the model which we build is too good to be true (i.,e gives predicted and actual output the same), then we should get suspicious and data leakage cannot be ruled out.\nAt that time, the model might be somehow memorizing the relations between feature and target instead of learning and generalizing it for the unseen data. \nSo, it is advised that before the testing, the prior documented results are weighed against the expected results.\nUsing EDA\nWhile doing the Exploratory Data Analysis (EDA), we may detect features that are very highly correlated with the target variable. Of course, some features are more correlated than others but a surprisingly high correlation needs to be checked and handled ca\nWe should pay close attention to those features. So, with the help of EDA, we can examine the raw data through statistical and visualization tools.\nHigh weight features\nAfter the completion of the model training, if features are having very high weights, then we should pay close attention. Those features might be leaky."
  },
  {
    "question": "How to fix the problem of Data Leakage?",
    "answer": "The main culprit behind this is the way we split our dataset and when. The following steps can prove to be very crucial in preventing data leakage:\nSelect the features such a way that they do not contain information about the target variable, which is not naturally available at the time of prediction.\nCreate a Separate Validation Set\nTo minimize or avoid the problem of data leakage, we should try to set aside a validation set in addition to training and test sets if possible. \nThe purpose of the validation set is to mimic the real-life scenario and can be used as a final step. \nBy doing this type of activity, we will identify if there is any possible case of overfitting which in turn can act as a caution warning against deploying models that are expected to underperform in the production environment.\nApply Data preprocessing Separately to both Train and Test subsets\nWhile dealing with neural networks, it is a common practice that we normalize our input data firstly before feeding it into the model. \nGenerally, data normalization is done by dividing the data by its mean value. More often than not, this normalization is applied to the overall data set, which influences the training set from the information of the test set and eventually it results in data leakage. \nHence, to avoid data leakage, we have to apply any normalization technique separately to both training and test subsets.\nProblem with the Time-Series Type of data\nWhen dealing with time-series data, we should pay more attention to data leakage. For example, if we somehow use data from the future when doing computations for current features or predictions, it is highly likely to end up with a leaked model. \nIt generally happens when the data is randomly split into train and test subsets. \nSo, when working with time-series data, we put a cutoff value on time which might be very useful, as it prevents us from getting any information after the time of prediction.\nTarget Leakage: This happens when predictors (features) include information that's only available after the target variable is known.\nExample: Predicting customer churn based on whether they canceled their subscription. The act of canceling is only known after they've churned, making it a leaky predictor."
  },
  {
    "question": "What is selection bias?",
    "answer": "Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes \nSampling bias is usually classified as a subtype of selection bias, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.\nDue to sampling bias, the probability distribution in the collected dataset deviates from its true natural distribution, which may affect ML models performance."
  },
  {
    "question": "Difference between supervised and unsupervised learning",
    "answer": "|Supervised|Unsupervised|\n\n\n|:-|:-|\n\n\n|Used for prediction|Used for analysis|\n\n\n|Labelled input data|Unlabelled input data|\n\n\n|Data need to be splitted into train/validation/test sets|No split required|\n\n\n|Used in Classification and Regression|Used for clustering, dimension reduction & density estimation|"
  },
  {
    "question": "Explain normal distribution of data",
    "answer": "Data can be distributed (spread out) in different ways,\nIt can be spread out more on the left (Left skew)\nMore on the right (Right Skew)\nIt can be all jumbled up\nBut there are many cases where the data tends to be around a central value with no bias left or right, and it gets close to a \"Normal Distribution\" like this:\nThe Normal Distribution has:\n - mean = median = mode\n - symmetry about the center\n - 50% of values less than the mean and 50% greater than the mean"
  },
  {
    "question": "What does it mean when distribution is left skew or right skew?",
    "answer": "In a right-skewed distribution, the tail on the right side is longer. This means most of the data is clustered on the left, with a few unusually large values pulling the average higher. Think of income distribution - most people earn less, but a few very high earners skew the average upwards.\nIn a left-skewed distribution, the tail on the left side is longer. This means most of the data is clustered on the right, with a few unusually small values pulling the average lower. An example could be exam scores where most students do well, but a few low scores bring down the average."
  },
  {
    "question": "What does the distribution looks like for the average time spend watching youtube per day?",
    "answer": "The distribution of average time spent watching YouTube per day is likely to be right-skewed.\nThis means that most people watch YouTube for a relatively short amount of time each day, while a smaller number of users watch for much longer durations. The tail of the distribution extends to the right, indicating the presence of these high-usage viewers"
  },
  {
    "question": "Expalin covariance and correlation",
    "answer": "Covariance and Correlation are two mathematical concepts which are commonly used in the field of probability and statistics. Both concepts describe the relationship between two variables.\n“Covariance” indicates the direction of the linear relationship between variables. “Correlation” on the other hand measures both the strength and direction of the linear relationship between two variables.\nIn case of High correlation, two sets of data are strongly linked together \n - Correlation is Positive when the values increase together, and\n - Correlation is Negative when one value decreases as the other increases"
  },
  {
    "question": "What is regularization. Why it is usefull?",
    "answer": "Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting.\nThe tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values.\nThere are two types of regularization as follows:\n - L1 Regularization or Lasso Regularization.\n L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weights.\n - L2 Regularization or Ridge Regularization.\n L2 Regularization or Ridge Regularization also adds a penalty to the error function. But the penalty here is the sum of the squared values of weights."
  },
  {
    "question": "What are confouding varaiables?",
    "answer": "In statistics, confounder is a variable that influences both the dependent variable and independent avriable.\nIf you are researeching whether a lack of exercise leads to weight gain. In this case 'lack of exercise' is independent variable and 'weight gain' is dependent variable. A confounding varaible in this case would be 'age' which affect both of these variables."
  },
  {
    "question": "Explain ROC curve and AUC",
    "answer": "ROC Curve (Receiver Operating Characteristic Curve) and AUC (Area Under the Curve) are tools used to evaluate the performance of a classification model, particularly when you want to understand how well the model separates two classes, such as \"spam\" and \"not spam.\"\nROC Curve:\n- What is the ROC Curve?\n - The ROC Curve is a graph that shows the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR) of a model at various thresholds.\n - True Positive Rate (TPR), also known as Recall, tells us how many of the actual positive cases (e.g., actual spam emails) were correctly predicted by the model.\n - False Positive Rate (FPR) tells us how many of the actual negative cases (e.g., non-spam emails) were incorrectly predicted as positive by the model.\n- How to Read the ROC Curve?\n - The X-axis represents the False Positive Rate (FPR), and the Y-axis represents the True Positive Rate (TPR).\n - A perfect model would reach the top left corner of the graph (high TPR and low FPR), indicating it correctly identifies all positives and has no false positives.\nAUC (Area Under the Curve):\n- What is AUC?\n - AUC stands for Area Under the ROC Curve. It provides a single number summary of the ROC Curve, which measures the overall ability of the model to distinguish between positive and negative classes.\n - AUC values range from 0 to 1:\n - An AUC of 0.5 means the model performs no better than random guessing.\n - An AUC close to 1 means the model has excellent performance, correctly distinguishing between the classes almost all the time.\nWhy are ROC and AUC Important?\n- Performance Comparison: ROC and AUC help compare different models or different settings of the same model to see which one performs better in distinguishing between classes.\n- Threshold Independence: Unlike accuracy, which depends on a specific threshold, the ROC Curve and AUC show how a model performs across all possible thresholds, giving a more complete picture of its performance.\n- Handling Imbalanced Data: They are particularly useful when the data is imbalanced (e.g., detecting fraud in banking transactions), as they focus on the ability to correctly classify the minority class.\nSummary:\nThe ROC Curve helps visualize the trade-off between detecting true positives and avoiding false positives at various thresholds, while the AUC gives a single score that summarizes the model's overall ability to separate classes. A higher AUC indicates a better model for distinguishing between the two classes.\n \nFor more detailed explaination please"
  },
  {
    "question": "Explain Precision-Recall Curve",
    "answer": "The Precision-Recall Curve is a tool used to evaluate the performance of a classification model, especially when dealing with imbalanced datasets where one class is much more common than the other.\nKey Concepts:\n- Precision:\n - Measures how many of the positive predictions made by the model are actually correct.\n - Example: If a model predicts 10 emails as spam and 8 are actually spam, the precision is 8 out of 10, or 80%.\n- Recall:\n - Measures how well the model finds all the actual positive cases.\n - Example: If there are 12 spam emails and the model correctly identifies 8 of them, the recall is 8 out of 12, or about 67%.\nWhat is the Precision-Recall Curve?\n- The Precision-Recall Curve is a graph that shows the trade-off between precision and recall at different thresholds for deciding whether something is positive (like spam) or negative (not spam).\n- X-axis: Recall (how many actual positives we caught).\n- Y-axis: Precision (how many of our positive predictions were correct).\nWhy is it Useful?\n- Better for Imbalanced Data: Unlike accuracy, which can be misleading with imbalanced data, the Precision-Recall Curve focuses on the performance of predicting the positive class, which is often the class of interest.\n \n- Visualize Trade-offs: The curve helps you see the trade-off between precision and recall. For example, if you want a model that catches almost all spam (high recall), the precision might drop (more false positives). The curve helps you find a good balance.\n- Optimize Model Performance: By looking at the curve, you can choose the best threshold that balances precision and recall for your specific needs. For example, in medical tests, you might want high recall to catch as many diseases as possible, even if it means more false alarms.\nSummary:\nThe Precision-Recall Curve is a visual tool that helps you understand how well a model identifies the positive cases (like spam or a disease) and balances making correct predictions with avoiding false positives, especially in situations where one class is much rarer than the other."
  },
  {
    "question": "What is TF-IDF?",
    "answer": "TF-IDF (Term Frequency-Inverse Document Frequency) is a method used in text analysis to determine how important a word is in a specific document compared to a whole collection of documents (called a corpus). It helps in identifying words that are most relevant to the content of a document.\nKey Concepts:\n- Term Frequency (TF):\n - This measures how often a word appears in a document. A higher term frequency means that the word is more significant within that document.\n - Simple Example: If the word \"cat\" appears 3 times in a document with 100 words, the term frequency for \"cat\" is 3/100 = 0.03.\n- Inverse Document Frequency (IDF):\n - This measures how rare or unique a word is across all documents in the corpus. A word that appears in many documents will have a low IDF, while a word that appears in only a few documents will have a high IDF.\n - Simple Example: If the word \"cat\" appears in only 1 out of 100 documents, its IDF is higher because it’s less common across the corpus.\n- TF-IDF Score:\n - This is the combination of TF and IDF. It gives a score that indicates how important a word is in a document, adjusted for how common it is across the entire corpus.\n - Words that are frequently used in a document but are rare in other documents get higher scores, highlighting their importance for that specific document.\nWhy TF-IDF is Useful:\n- Identifies Key Terms: TF-IDF helps in identifying the most important words in a document, which are often key topics or themes.\n- Improves Search Results: In search engines, TF-IDF can be used to rank documents based on their relevance to a user's query by focusing on unique and significant words.\n- Text Classification and Clustering: It helps in categorizing or grouping documents by identifying unique terms that define different categories.\nIn Simple Terms:\nTF-IDF helps find words that are important in a document but not too common across other documents. It’s a tool to highlight what makes a document unique, which is useful for search engines, text analysis, and understanding the content better."
  },
  {
    "question": "Python or R- which one would you prefer for text analytics?",
    "answer": "We will p\nWe can use pandas library which has easy to use data structures and high performance data analysis tools\nR is more suitable for ML than text analytics\nPython is faster for all types of text analytics."
  },
  {
    "question": "What are Eigenvectors and Eigenvalues?",
    "answer": "In linear algebra, an eigenvector is a special vector that, when a linear transformation is applied to it, only changes in scale (gets stretched or shrunk) but not in direction. \nThe eigenvalue associated with that eigenvector is the factor by which it is scaled.\nWhy are they important?\nEigenvectors and eigenvalues reveal the underlying structure and behavior of linear transformations. They have numerous applications across various fields:\nImage compression: Eigenvectors can be used to represent images efficiently, leading to compression techniques like Principal Component Analysis (PCA).\nFacial recognition: Eigenfaces, derived from eigenvectors, are used to represent and recognize faces.\nPageRank algorithm: Google's PageRank algorithm uses eigenvectors to rank web pages based on their importance.\nPhysics and engineering: Eigenvalues and eigenvectors are used to analyze vibrations, stability, and other properties of systems.\nMachine learning: They are used in dimensionality reduction techniques, clustering algorithms, and understanding the behavior of neural networks."
  },
  {
    "question": "Explain the scenario where both false positive and false negative are equally important",
    "answer": "- Medical Diagnosis (e.g., Cancer Screening)\nFalse Positive: A patient is told they have cancer when they don't. This leads to unnecessary anxiety, invasive procedures, and potential side effects from treatment.\nFalse Negative: A patient with cancer is told they are healthy. This delays crucial treatment, potentially allowing the disease to progress and worsen the prognosis.\n- Fraud Detection\nFalse Positive: A legitimate transaction is flagged as fraudulent. This inconveniences the customer, potentially disrupting their business or causing reputational damage.\nFalse Negative: A fraudulent transaction goes undetected. This results in financial loss for the business or individual, and can enable further fraudulent activity."
  },
  {
    "question": "Why feature scalling is required in Gradient Descent Based Algorithms",
    "answer": "Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below:\nThe presence of feature value X in the formula will affect the step size of the gradient descent. \nThe difference in ranges of features will cause different step sizes for each feature. \nTo ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\nHaving features on a similar scale can help the gradient descent converge more quickly towards the minima."
  },
  {
    "question": "Why feature scalling is required in distance Based Algorithms",
    "answer": "Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\nFor example, let’s say we have data containing high school CGPA scores of students (ranging from 0 to 5) and their future incomes (in thousands Rupees):\nSince both the features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we do not want our algorithm to be biassed towards one feature.\nThe\nThe effect of scaling is conspicuous(clearly visible) when we compare the Euclidean distance between data points for students A and B, and between B and C, before and after scaling as shown below:\n Distance AB before scaling => \n Distance BC before scaling => \n Distance AB after scaling => \n Distance BC after scaling => \nScaling has brought both the features into the picture and the distances are now more comparable than they were before we applied scaling."
  },
  {
    "question": "Why feature scaling not required in tree based algorithms",
    "answer": "Imagine you're sorting a pile of apples and oranges into two baskets. You could sort them by color (red vs. not red) or by weight (heavy vs. light).\nTree-based algorithms work like this: They make decisions based on thresholds or cut-offs for each feature (like color or weight). They ask questions like: \"Is this fruit red?\" or \"Is this fruit heavier than 1 pound?\".\nFeature scaling doesn't matter here: \nColor: It doesn't matter if we represent \"red\" as the number 1 and \"not red\" as 0, or if we use some other scale. The decision (is it red or not?) remains the same.\nWeight: Even if we change the units from pounds to kilograms, the relative heaviness of the fruits doesn't change. A heavy apple will still be heavier than a light orange, regardless of the units.\nIn simpler terms:\nTree-based algorithms care about the order or ranking of the data, not the exact numerical values. \nScaling the features changes the numbers but doesn't change their order. A heavy fruit stays heavy, and a light fruit stays light, no matter what units we use.\nThis makes tree-based algorithms invariant to monotonic transformations of the features (transformations that preserve the order).\nThe\nExceptions:\nSome implementations: Certain libraries or specific algorithms within the tree-based family might be sensitive to the scale of the features due to implementation details. It's always good to check the documentation or experiment to be sure.\nDistance-based calculations: If your tree-based algorithm involves any calculations based on distances between data points, then scaling might be necessary to ensure all features contribute equally to the distance calculation."
  },
  {
    "question": "Explain the difference between train, validation and test set",
    "answer": "Training set is used for model training \nValidation set is used for model fine tuning (tune the model's hyperparameters)\nTest set is used for model testing. i.e. evaluating the models predictive power and generalization."
  },
  {
    "question": "What is Naive Bayes algorithm?",
    "answer": "The Naive Bayes algorithm is a simple but surprisingly effective classification algorithm in machine learning. It's based on Bayes' Theorem, which deals with conditional probabilities.\nIn simpler terms:\nImagine you're trying to decide if an email is spam or not. Naive Bayes looks at the words in the email and asks, \"If an email is spam, how likely is it to contain these words?\" It then does the same for non-spam emails. Finally, it compares these probabilities to make its best guess about whether the email is spam.\nThe \"Naive\" Assumption:\nThe key assumption here is that all the words in the email are independent of each other. This means it doesn't consider the order of the words or any relationships between them. It's a bit of a simplification, but it works surprisingly well in practice, especially for text classification tasks.\nWhy is it useful?\nSimple and fast: Naive Bayes is easy to understand and implement, and it can be trained very quickly on large datasets.\nHandles high-dimensional data: It's good at dealing with lots of features (like all the different words in an email).\nWorks well with text: It's often used for tasks like spam filtering, sentiment analysis, and document classification.\nLimitations:\nThe \"naive\" assumption: In reality, features are often not completely independent, which can affect the accuracy in some cases.\nSensitive to the absence of features: If a feature is absent in the training data but appears in new data, it can throw off the model.\nOverall:\nNaive Bayes is a great starting point for classification tasks, especially when you need something simple, fast, and effective. While its \"naive\" assumption might seem limiting, it often performs surprisingly well, especially for text-related problems."
  },
  {
    "question": "What is the difference between MLOps and DevOps?",
    "answer": "MLOps & DevOps have a lot of things in common. However, DevOps include developing and deploying the software application code in production and this code is usually static and does not change rapidly.\nMLOps on the other side also includes developing and deploying the ML code in production. However, here the data changes rapidly and the up-gradation of models has to happen more frequently than typical software application code."
  },
  {
    "question": "What are the risks associated with Data Science & how MLOps can overcome the same?",
    "answer": "In Data Science, several risks can impact projects, such as data quality issues, model deployment challenges, model performance degradation, lack of reproducibility, security concerns, and scalability issues. \nMLOps (Machine Learning Operations) helps mitigate these risks by:\n- Automating Data Quality Checks and Monitoring: Ensures consistent data quality and detects data drift, which helps maintain model accuracy over time.\n- Streamlining Model Deployment and Environment Consistency: Uses automated pipelines and containerization to deploy models smoothly across different environments, reducing errors and ensuring reliability.\n- Continuous Monitoring and Automated Retraining: Tracks model performance in real-time and triggers retraining when necessary, preventing performance degradation and adapting to new data patterns.\n- Improving Reproducibility and Collaboration: Incorporates version control for code, data, and models, making it easier to reproduce results and collaborate across teams.\n- Enhancing Security and Compliance: Implements strict access controls, auditing, and encryption to protect data and models, ensuring compliance with regulations.\n- Optimizing Scalability and Resource Management: Supports scalable infrastructure and provides insights into cost and resource use, ensuring models can handle large volumes and operate efficiently.\nBy implementing MLOps, we can address these risks effectively, leading to more robust, reliable, and scalable machine-learning models."
  },
  {
    "question": "What are the differences between XGBoost and Random Forest Model",
    "answer": "| Feature | XGBoost | Random Forest |\n\n\n|------------------------------|----------------------------------------------|---------------------------------------------|\n\n\n| Technique | Boosting Technique: Builds trees sequentially, where each tree corrects the errors of the previous one. | Bagging Technique: Builds trees independently in parallel and aggregates their results. |\n\n\n| Performance | Generally provides higher accuracy due to its boosting nature. | May have lower accuracy compared to boosted models. |\n\n\n| Speed | Faster training times due to optimized algorithms | Slower training times as it trains many trees in parallel. |\n\n\n| Overfitting Handling | Includes regularization parameters to reduce overfitting. | Uses averaging of results to mitigate overfitting |\n\n\n| Model Complexity | More complex to tune with multiple hyperparameters. | Simpler to tune with fewer hyperparameters. |\n\nIn summary, XGBoost is often p"
  },
  {
    "question": "Please explain p-value to someone non-technical",
    "answer": "A p-value is a number that helps us understand if the results we see in an experiment or study are meaningful or if they might have happened just by chance.\nImagine you're playing a game of chance, like flipping a coin. You suspect the coin might be rigged to land on heads more often, so you decide to test it.\nThe \"normal\" assumption (null hypothesis): The coin is fair, and there's a 50/50 chance of getting heads or tails.\nYour experiment: You flip the coin 100 times and get 60 heads. Hmm, that seems a bit high...\nThe p-value comes in: It tells you, \"If the coin was fair, how likely is it that you'd get a result as extreme (or more extreme) as 60 heads out of 100 flips, just by random chance?\"\nA small p-value (e.g., 0.01) means it's very unlikely to get such a result with a fair coin. This makes you suspicious – maybe the coin is rigged!\nA large p-value (e.g., 0.50) means it's quite likely to get such a result even with a fair coin. So, you don't have strong evidence to say the coin is rigged.\nIn simpler terms:\nThe p-value is like a \"surprise meter\". The lower the p-value, the more surprised you'd be to see your results if the \"normal\" assumption were true.\nIt helps you decide whether your results are strong enough to challenge the \"normal\" assumption or if they could just be due to random luck.\nImportant Note:\nThe p-value doesn't prove anything. It just gives you a measure of how surprising your results are.\nIt's up to you to decide what level of \"surprise\" is enough to make you doubt the \"normal\" assumption. \nIf the p-value is less than or equal to alpha(0.05), it means your results are statistically significant, and you have enough evidence to reject the null hypothesis. In other words, your data suggests that the \"normal\" assumption is probably not true."
  },
  {
    "question": "Average comments per month has dropped over three-month period, despite consistent growth after a new launch. What metric would u investigate?",
    "answer": "To investigate the drop in average comments per month despite consistent growth after a new launch, I would examine:\n- Engagement per User: Assess if the average number of comments per active user has decreased. Even with user growth, fewer comments per user could explain the overall decline.\n- Content Type Analysis: Identify if the types of content driving growth are different from those that typically generate comments. Growth could be from content that attracts more views or reads, but not comments.\n- New vs. Returning User Behavior: Check if new users, who might contribute to growth, are engaging less in commenting than returning users. This could indicate that while more people are joining, they aren't commenting as much.\n- User Experience and UI Changes: Review if changes in the user interface or user experience since the launch might have unintentionally made it harder or less appealing to comment.\n- Sentiment and Feedback: Analyze user feedback for any negative sentiment or issues related to commenting since the new growth phase.\nBy focusing on these areas, we can identify the specific reasons behind the decline in comments despite overall platform growth."
  },
  {
    "question": "A PM tells you that a weekly active user metric is up by 5% but email notification open rate is down by 2%. WHat would you investigate to dignose this problem?",
    "answer": "Email open rate is calculated by dividing the number of emails opened by the number of emails sent minus any bounces. A good open rate is between 17-28%2. Email notification open rate is a type of email open rate that measures how many users open an email that notifies them about something.\nWeekly active user metric (WAU) is a measure of how many users are active on a website or app in a given week. It can be influenced by many factors, such as user acquisition, retention, engagement and churn.\nTo diagnose the problem of WAU being up but email notification open rate being down, you might want to investigate:\nHow are you defining active users? Are they performing meaningful actions on your website or app that indicate engagement and loyalty?\nHow are you segmenting your users based on their behavior, p\nHow are you optimizing your email subject lines, preheaders, sender names and content to capture attention and interest? Are you using clear and compelling calls to action?\nHow are you testing and measuring your email performance? Are you using tools like A/B testing, analytics and feedback surveys to improve your email strategy?"
  },
  {
    "question": "Explain data drift problem in machine learning",
    "answer": "Data drift is a common problem in machine learning that occurs when the statistical properties of the input data change over time. This change can lead to a decrease in the performance of machine learning models because the model is no longer receiving the same kind of data it was trained on. \nKey Points to Explain Data Drift:\n- Definition of Data Drift:\n - Data drift \n- Types of Data Drift:\n - Covariate Drift: This occurs when the distribution of the input features (independent variables) changes. For example, if a model is predicting sales based on weather data, and there is a shift in climate patterns, the relationship between weather and sales could change.\n - Prior Probability Shift: This type of drift happens when the distribution of the target variable (dependent variable) changes. For instance, in a spam detection model, if the proportion of spam emails decreases over time, the model might need adjustment.\n - Concept Drift: This happens when the relationship between the input features and the target variable changes. For example, a model predicting credit card fraud may experience concept drift if fraudsters change their tactics over time.\n- Why Data Drift is a Problem:\n - Decreased Model Accuracy: Models trained on historical data may perform poorly on new data that has drifted. This can lead to incorrect predictions and reduced effectiveness of the model.\n - Impact on Business Decisions: In many applications, such as finance, healthcare, and marketing, decisions based on outdated models can lead to significant financial loss, poor customer experiences, or even safety issues.\n- Detecting Data Drift:\n - Statistical Tests: Techniques such as Kolmogorov-Smirnov test, Chi-square test, or Jensen-Shannon divergence can be used to detect changes in data distributions.\n - Monitoring Performance Metrics: Regularly tracking model performance metrics (like accuracy, precision, recall) over time can indicate data drift if there’s a consistent decline.\n - Data Visualization: Visual methods such as histograms, scatter plots, or density plots can help visualize changes in data distributions over time.\n- Mitigating Data Drift:\n - Regular Model Retraining: Regularly retraining models with new data helps to ensure they stay up-to-date with the latest trends and patterns.\n - Continuous Monitoring: Implementing systems to continuously monitor incoming data and model performance allows for early detection of drift.\n - Adaptive Learning: Techniques like online learning or incremental learning can help models adapt to changes by learning from new data as it becomes available.\n - Feature Engineering Updates: Revisiting and potentially redefining features to align with the new data distributions can help in maintaining model accuracy.\nReal-World Example of Data Drift:\nImagine a retail company that uses a machine learning model to predict customer p\nConclusion:\nData drift is a critical issue in maintaining the accuracy and reliability of machine learning models. Regular monitoring, retraining, and adaptation strategies are essential to ensure models remain effective as real-world conditions change."
  },
  {
    "question": "Explain transformer architecture",
    "answer": "The Transformer architecture is a neural network model designed for natural language processing tasks, like translation, summarization, and text generation. It was introduced in the paper \"Attention is All You Need\" by Google in 2017 and has since become the foundation for many advanced NLP models, including BERT and GPT.\nKey Components of the Transformer:\n- Self-Attention Mechanism:\nThe core innovation of the Transformer is its self-attention mechanism. This mechanism allows the model to weigh the importance of each word in a sentence relative to every other word when generating a representation. For example, in the sentence \"She saw a dog in the park,\" the word \"she\" has a different relation to \"saw\" than \"dog\" or \"park.\" Self-attention helps the model understand these relationships dynamically.\n- Multi-Head Attention:\nMulti-head attention enables the model to learn different aspects of word relationships simultaneously by using multiple \"attention heads.\" Each head learns a unique representation of the sentence, which, when combined, provides a more comprehensive understanding.\n- Positional Encoding:\nSince the Transformer processes all words in parallel (not sequentially), it uses positional encoding to inject information about the position of words into their embeddings. This ensures the model understands the order of words, which is crucial for meaning.\n- Feed-Forward Neural Networks:\nEach layer in the Transformer also includes a feed-forward neural network that processes the attention outputs to create more complex representations of the input data.\n- Stacked Layers:\nThe Transformer is composed of several layers of these attention and feed-forward modules. The stacking allows the model to learn increasingly abstract and complex features at each layer.\nAdvantages of the Transformer:\nParallelization: Unlike older models like RNNs, Transformers can process all words in a sentence simultaneously. This makes training much faster and more scalable, especially with large datasets.\nLong-Range Dependency Handling: Transformers excel at capturing relationships between distant words in a sentence, which is essential for understanding context in lengthy texts.\nWhy Transformers Are Important:\nThe ability of Transformers to handle context and relationships in a sentence far surpasses previous models, making them ideal for a wide range of NLP tasks. Their architecture has become the standard in the field, driving innovations in machine translation, text summarization, and language understanding."
  },
  {
    "question": "Explain the difference between prediction and forecasting",
    "answer": "If you think of it like a detective story, prediction is about figuring out who committed the crime based on the evidence. It could be something that happened in the past. \nForecasting is more like trying to prevent a crime before it happens. We look for patterns and clues to anticipate what might occur in the future.\nIn the context of my work with time series models, prediction might be used to identify any current anomalies or issues in our system based on the data we're receiving. Forecasting, however, is where we really leverage the power of time series. By analyzing past trends, we can forecast future system loads and proactively scale our resources to avoid any disruptions.\nThe main difference is the time element. Forecasting is always about the future, while prediction can be about any point in time. Also, forecasting usually involves time-series data, where we track something over time, like sales figures or stock prices."
  }
]